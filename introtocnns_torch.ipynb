{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "introtocnns-torch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olahfemi/HackExpo/blob/master/introtocnns_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "9jlE2IPptpPX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction to Convolutional Neural Networks\n",
        "\n",
        "Most of the success achieved so far in the field of Deep Learning can be attributed to the development of a special class of Artificial Neural Networks called \"Convolutional Neural Networks.\" They were initially developed by Yann Lecun et al in 1998. At inception, they were used primarily for image classifaction, however, their application has evolved and has become the primary building block for virtually every field of Deep Learning, from Image Recognition, Detection, Segmentation to Speech Synthesis and Recognition, Face Detection and Recognition, Sequence to Sequence Learning, Generative Adversarial Networks, Reinforcement Learning, Variational AutoEncoders, Flow Models and much more. \n",
        "\n",
        "They are generally well suited for tasks involving high dimensional unstructured data. However, they are not well suited for structured data like tables. \n",
        "\n",
        "While CNNs are applicable to almost any field of Deep Learning, we shall explain both the theoretical details and practical applications from the viewpoint of Image Classification, however, the lessons learned here can be easily transferred to almost any other field of Artificial Intelligence.\n",
        "\n",
        "\n",
        "To fully understand the way CNNs work, a basic understanding of the structure of Images is very helpful.\n",
        "\n",
        "#Structure of Images\n",
        "![Arrangement of Pixels in an Image](https://github.com/johnolafenwa/CNNLecture/raw/master/images/img_arrangement.png)\n",
        "\n",
        "This is a cat. The image looks beautiful to the eye but in computer memory, it is purely 3 Dimensional Array of Numbers. Images are all made up of pixels, these pixels are numbers ranging from 0 to 255. A Standard RGB image is made up of three layers of pixels as showb above. The RED layer, the Green Layer and the Blue Layer. Often and in the rest of this document, we shall use the word \"channels\" instead of layers. \n",
        "In each channel(layer) we have pixels arranged in grids. The manner in which these pixels are arranged will determine the way the image looks like. One important thing to note is that in each channel , the value of each pixel represents the intensity of the pixel. For example, in the red channel, a value of 255 represents absolute red while a much lesser value indicates a lower intensity.\n",
        "\n",
        "Consider a 4 x 4 Image made of 3 channels, each of the channel will be a 4 x 4 grid of pixels like below\n",
        "\n",
        "![4 x4 grid of pixels](https://github.com/johnolafenwa/CNNLecture/raw/master/images/pixelgrid.png)\n",
        "\n",
        "As you can see above, the lower values have a color tending towards black while the higher values are lighter. Combining a this structure across 3 difference channels forms the beautiful pictures we see daily.\n",
        "\n",
        "In summary, an Image is made of channels, and each channel is a 2 Dimensional grid of pixel values. GrayScale (Black and White) pictures have only a single channel while RGB images have 3 Channels.\n",
        "\n",
        "Convolutional Neural Networks takes great advantage of this structure to detect patterns in an image.\n",
        "\n",
        "\n",
        "#HOW CONVOLUTIONAL NEURAL NETWORKS WORKS\n",
        "\n",
        "Humans recognize objects through learned patterns. You know a car by patterns such as the wheels, the windscreen, the rear mirror etc.\n",
        "\n",
        "![Car](https://github.com/johnolafenwa/CNNLecture/raw/master/images/car.jpg)\n",
        "\n",
        "Humans learn this patterns implicitly from experience and once we detect these patterns in new scenes, we use them to make decisions. \n",
        "\n",
        "The traditional approach to build a computer vision system that can tell cars apart from animals is to hardcode this features directly into the system such that we explicitly define different parts of a car and animal and their pixel arrangements. However, the number of such possible patterns is very large, due to the variation in the way these objects appear. Convolutional Neural Networks solves this problem by learning the patterns that distinguishes different classes of items directly from data. \n",
        "By presenting a 1000 pictures of cars to a convolutional neural network, it can implicitly learn all the instrisic patterns that makes a car look like a car. In this light, convolutional neural networks are \"Pattern Detectors\". Once they have learned the neccessary patterns, they can then use these patterns to classify new images correctly.\n",
        "\n",
        "\n",
        "# CNN KERNELS AND CHANNELS\n",
        "Kernels are the most important parameters in a CNN, they represent the patterns the network learns. A kernel is fixed size parameter, common sizes are 1 x 1 , 2 x 2, 3 x 3 and 5 x 5 kernels.\n",
        "To illistrate what kernels are and how they work, let's consider a classic horizontal line detector kernel below.\n",
        "Here are two images\n",
        "\n",
        "![image 1](https://github.com/johnolafenwa/CNNLecture/raw/master/images/img1.png)   \n",
        "![image 2](https://github.com/johnolafenwa/CNNLecture/raw/master/images/img3.png)\n",
        "\n",
        "The first image clearly has a horizontal red line, the second image on the other hand does not contain a horizontal line. \n",
        "\n",
        "This is clear to the human eye, below is a kernel that can automatically detect this.\n",
        "![kernel](https://github.com/johnolafenwa/CNNLecture/raw/master/images/line_detector.png)\n",
        "\n",
        "We shall compute the dot product of the kernel with each of the two images.\n",
        "\n",
        "## IMAGE 1 ACTIVATION\n",
        "activation = (200 x 1.5) + (200 x 1.5) + (0 x -1.5) + (0 x -1.5) = 600\n",
        "\n",
        "With an activation value of 600, we can be very confident that a horizontal line was detected in the image.\n",
        "\n",
        "## IMAGE 2 ACTIVATION\n",
        "activation =  (200 x 1.5) + (0 x 1.5) + (200 x -1.5) + (0 x -1.5) = 0\n",
        "\n",
        "Here the activation value is zero, because no horizontal line was detected.\n",
        "\n",
        "In a convolution operation, multiple kernels are applied on the same image, this helps the network rely on more than one pattern. In the example above, we hardcoded the horizontal line detector, however, CNNs are trained to come up with their own kernels. The number of kernels applied at each layer of the convolution operation is described as the number of output channels.\n",
        "\n",
        "\n",
        "The convolution operation will be applied across all dimensions of the image to produce multiple activation values as shown below:\n",
        "\n",
        "![convolution operation](http://deeplearning.net/software/theano/_images/no_padding_no_strides.gif)\n",
        "\n",
        "Here a 3 x 3  convolution operation is applied with a stride of 1 on a 4 x 4 image to produce a 2 x 2 activation map. This 2 x 2 activation map is used as the input image in the next layers.\n",
        "\n",
        "## PADDING\n",
        "Often, we want the activation map to retain the size of the input image, unlike above where the size was halved. To achieve this, we can simply pad the image with zeros. Below is an example\n",
        "\n",
        "![padding](http://deeplearning.net/software/theano/_images/same_padding_no_strides.gif)\n",
        "\n",
        "As you can see here the output remains 5 x 5, the size of the input image.\n",
        "\n",
        "## STRIDE\n",
        "Sometimes, we want to deliberately reduce the size of the activation map, this can be achieved by using a stride greater than 1. Example below.\n",
        "\n",
        "![stride](http://deeplearning.net/software/theano/_images/padding_strides_odd.gif)\n",
        "\n",
        "Here the output is exactly half the input due to a stride of 2.\n",
        "\n",
        "# CODING TIME\n",
        "The theory of convolutions is quite a lot, in this treatise, i have only covered the minimum required. \n",
        "Additional resources to learn about convolutions are treated at the end of this notebook. Now, we shall apply convolutions to train Image Classification Models.\n",
        "\n",
        "\n",
        "While the logic of convolutions can be more complex, a number of DeepLearning frameworks has made it much easier.\n",
        "\n",
        "Here we shall be using TorchFusion, a modern Deep Learning framework built on Pytorch. We shall install both frameworks below."
      ]
    },
    {
      "metadata": {
        "id": "UITebslvk88p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch --upgrade\n",
        "!pip3 install torchfusion --upgrade"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ALTR_Bso_ewD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using TorchFusion and Pytorch, we shall design and train a Convolutional Neural Network that can accurately tell a person's profession based on their dressing.\n",
        "\n",
        "The dataset we shall use to train the model is \"IdenProf\", a dataset of 11 000 pictures of professionals spread across ten categories including: Chef, Doctor, Engineer, Farmer, Firefighter, Judge, Mechanic, Pilot, Police, Waiter\n",
        "\n",
        "Download the dataset from https://github.com/OlafenwaMoses/IdenProf/releases/download/v1.0/idenprof-jpg.zip and extract the data.\n",
        "The dataset is composed of two main folders, the train and test folder. Each folder contains sub-folders for each of the ten classes.\n",
        "\n",
        "Next, import the neccessary packages"
      ]
    },
    {
      "metadata": {
        "id": "7vZ1njWWIfJs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://github.com/OlafenwaMoses/IdenProf/releases/download/v1.0/idenprof-jpg.zip\n",
        "!unzip idenprof-jpg.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SwZHKMYqAAqg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchfusion.layers import *\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torchfusion.learners import StandardLearner\n",
        "from torchfusion.metrics import *\n",
        "from torchfusion.datasets import *\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "TRAIN_FOLDER = \"./idenprof/train\"\n",
        "TEST_FOLDER = \"./idenprof/test\"\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zyrqih1Vc73W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below we define a CNN Model using the Keras Functional API"
      ]
    },
    {
      "metadata": {
        "id": "sZnl7-zY8_TI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self,in_filters, out_filters,stride=1):\n",
        "      super(ConvLayer,self).__init__()\n",
        "      \n",
        "      self.net = nn.Sequential(\n",
        "      Conv2d(in_filters,out_filters,kernel_size=3,stride=stride),\n",
        "      BatchNorm2d(out_filters),\n",
        "      nn.ReLU()\n",
        "      )\n",
        "    \n",
        "    def forward(self,x):\n",
        "      return self.net(x)\n",
        "      \n",
        "      return output\n",
        "    \n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self,num_classes=10):\n",
        "      super(SimpleNet,self).__init__()\n",
        "      \n",
        "      self.net = nn.Sequential(\n",
        "      \n",
        "          ConvLayer(3,16,stride=2),\n",
        "          ConvLayer(16,32),\n",
        "          ConvLayer(32,32),\n",
        "          \n",
        "          ConvLayer(32,32,stride=2),\n",
        "          ConvLayer(32,64),\n",
        "          ConvLayer(64,64),\n",
        "          \n",
        "          ConvLayer(64,64,stride=2),\n",
        "          ConvLayer(64,128),\n",
        "          ConvLayer(128,128) ,\n",
        "          nn.Dropout(0.5),\n",
        "          \n",
        "          GlobalAvgPool2d(),\n",
        "          \n",
        "          Linear(128,10)\n",
        "      )\n",
        "    def forward(self,x):\n",
        "      return self.net(x)\n",
        "\n",
        "model = SimpleNet()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "  \n",
        "learner = StandardLearner(model)\n",
        "\n",
        "print(learner.summary((3,224,224)))\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z_kYDLbrDEVA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We need to load the IndenProf Dataset"
      ]
    },
    {
      "metadata": {
        "id": "RBtJaKBRFF6F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomCrop(224,padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "train_loader = imagefolder_loader(transform=train_transforms,batch_size=32,shuffle=True,root=TRAIN_FOLDER)\n",
        "test_loader = imagefolder_loader(transform=test_transforms,shuffle=False,batch_size=32,root=TEST_FOLDER)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6bTGzm3zb2_S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below the model is trained\n",
        "\n",
        "# Note\n",
        "Due to an issue with Google Colab and PIL, before running the training, go to Runtime and click restart runtime, else you will run into an error."
      ]
    },
    {
      "metadata": {
        "id": "fWhDw7ZqKS4f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "train_metrics = [Accuracy()]\n",
        "test_metrics = [Accuracy()]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    learner.train(train_loader,train_metrics=train_metrics,optimizer=optimizer,loss_fn=loss_fn,model_dir=\"./my-torch-models\",test_loader=test_loader,test_metrics=test_metrics,num_epochs=200,batch_log=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KZr8o4YdXKMj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the setup above, the model achieves a accuray of about 80% after 63 epochs. The best model is already saved in our model directory. Using this trained model, we can easily predict the class of new images.\n",
        "\n",
        "Below we download some images to use for testing."
      ]
    },
    {
      "metadata": {
        "id": "NEyrGt4tp3W_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://github.com/johnolafenwa/CNNLecture/raw/master/images/testimages.zip\n",
        "!unzip testimages.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYuJPg9jqBEC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below is the full script to re-define the network, load the trained model and predict the classes of new images."
      ]
    },
    {
      "metadata": {
        "id": "6ak6-DuPg4Qg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchfusion.layers import *\n",
        "import torch.nn as nn\n",
        "from torchfusion.learners import StandardLearner\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self,in_filters, out_filters,stride=1):\n",
        "      super(ConvLayer,self).__init__()\n",
        "      \n",
        "      self.net = nn.Sequential(\n",
        "      Conv2d(in_filters,out_filters,kernel_size=3,stride=stride),\n",
        "      BatchNorm2d(out_filters),\n",
        "      nn.ReLU()\n",
        "      )\n",
        "    \n",
        "    def forward(self,x):\n",
        "      return self.net(x)\n",
        "      \n",
        "      return output\n",
        "    \n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self,num_classes=10):\n",
        "      super(SimpleNet,self).__init__()\n",
        "      \n",
        "      self.net = nn.Sequential(\n",
        "      \n",
        "          ConvLayer(3,16,stride=2),\n",
        "          ConvLayer(16,32),\n",
        "          ConvLayer(32,32),\n",
        "          \n",
        "          ConvLayer(32,32,stride=2),\n",
        "          ConvLayer(32,64),\n",
        "          ConvLayer(64,64),\n",
        "          \n",
        "          ConvLayer(64,64,stride=2),\n",
        "          ConvLayer(64,128),\n",
        "          ConvLayer(128,128) ,\n",
        "          nn.Dropout(0.5),\n",
        "          \n",
        "          GlobalAvgPool2d(),\n",
        "          \n",
        "          Linear(128,10)\n",
        "      )\n",
        "    def forward(self,x):\n",
        "      return self.net(x)\n",
        "\n",
        "model = SimpleNet()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "  \n",
        "  \n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "learner = StandardLearner(model)\n",
        "learner.load_model(\"my-torch-models/best_models/model_43.pth\")\n",
        "\n",
        "IMAGES_FOLDER = \"./testimages\"\n",
        "\n",
        "IMAGES = os.listdir(IMAGES_FOLDER)\n",
        "PREDICTIONS = []\n",
        "class_map = {0:\"Chef\", 1:\"Doctor\", 2:\"Engineer\", 3:\"Farmer\", 4:\"Firefighter\", 5:\"Judge\", 6:\"Mechanic\", 7:\"Pilot\", 8:\"Police\", 9:\"Waiter\"}\n",
        "\n",
        "for image_file in IMAGES:\n",
        "    image_file = os.path.join(IMAGES_FOLDER,image_file)\n",
        "    img = Image.open(image_file).convert(\"RGB\")\n",
        "    \n",
        "    #perform preprocessing\n",
        "    img = image_transforms(img)\n",
        "    \n",
        "    #add batch dimension\n",
        "    img = img.unsqueeze(0)\n",
        "    \n",
        "    prediction = learner.predict(img).argmax().item()\n",
        "    \n",
        "    #append predictions\n",
        "    PREDICTIONS.append(prediction)\n",
        "    \n",
        "    \n",
        "\n",
        "#print predictions:\n",
        "\n",
        "for image_file,prediction in zip(IMAGES,PREDICTIONS):\n",
        "  \n",
        "    class_name = class_map[prediction]\n",
        "    \n",
        "    print(\"File: {} , Class Prediction: {} Class Name: {}\".format(image_file,prediction,class_name))\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c5naTFsVxS1L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ABOUT THE WRITER\n",
        "John Ishola Olafenwa is the CTO of [DeepQuestAI](https://deepquestai.com). He is a Deep Learning researcher, a machine learning engineer and the creator of [TorchFusion](https://github.com/johnolafenwa/TorchFusion)\n",
        "\n",
        "You can reach him on twitter [@johnolafenwa](https://twitter.com/johnolafenwa)\n",
        "\n",
        "Email:  johnolafenwa@gmail.com\n",
        "\n",
        "![John Olafenwa](https://deepquestai.com/about/john.png)\n"
      ]
    }
  ]
}